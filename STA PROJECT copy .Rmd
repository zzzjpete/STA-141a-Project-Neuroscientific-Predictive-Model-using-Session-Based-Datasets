---
title: "Neuroscientific Predictive Model using Session-Based Datasets"
author: "Peter Zhang"
date: "2023-05-10"
output: html_document
---

# Abstract
In this project, we first conducted exploratory data analysis by generating a table that shows features across sessions: number of neurons, number of trials, mouse name, brain area, stimuli conditions, and success rate. Next, we examined neural activities in session five by calculating the peak firing rate for each neuron. A histogram is generated to help visualize the result. We also explored the average number of failure rates in each session. In Changes Across Trials, we focused on finding the changes in the variable “feedback_type” across trials in each session.  In terms of Homogeneity and heterogeneity across sessions and mice, we focused on finding the similarities and differences between mice across sessions, particularly trial outcomes(success or failure) After we gathered enough useful information, we extract data with shared patterns across sessions to create a sub-data frame of the entire dataset, and the purpose of doing so is to enhance the performance of the prediction performance model later on. Five are variables are chose to construct the sub-data frame: “contrast_left”, “contrast_right”, 'spks_mean', “spks_sd”, “total_neurons”. Then, we perform K-means clustering method to the sub-data frame, and lastly, we perform Principal Component Analysis.

# Introduction
In this project, we will be using a subset of data collected by Steinmetz et al. (2019) which aims at predicting the outcome (feedback type) based on the randomly presented visual stimuli to the four mice.
This subset of data is structured to contain 18 sessions with each session containing multiple trials. Inside each trial, we have five variables: "feedback_type", "contrasts", "time", "spks", and "brain_area". "feedback_type" represents the type of the feedback, 1 for success and -1 for failure. "contrast_left" represents contrast of the left stimulus. "contrast_right" represents the contrast of the right stimulus. "time" refers to centers of the time bins for "spks".
"spks" represents the numbers of spikes of neurons in the visual cortex in time bins defined in "time". "brain_area" represents the area of the brain where each neuron lives. The primary objective of this project is to build a predictive model that predicts the outcome (feedback type) of each trial using the neural activity data (spike trains in "spks"), along with the stimuli (the left and right contrasts).

# Exploratory data analysis

```{r, echo=FALSE}
# load the list into the rmarkdown
session=list()
for(i in 1:18){
  session[[i]]=readRDS(paste('~/Downloads/sessions/session',i,'.rds',sep=''))
  # print(session[[i]]$mouse_name)
  # print(session[[i]]$date_exp)
}
```

### i) Describing data structures across sessions
In the summary table created below, five variables have been chosen to assess data structures across sessions: mouse_name, brain_area, neuron_counts, trial_counts, and success_rate. These variables have been selected because they provide the most straightforward information for the audience to understand what the data is about. 'success_rate' represents the propotion of trials in which the mouse correctly identified the stimulus. 'mouse_name' represents the mouse involved in each session. 'brain_area' represents the number of unique brain areas involved in each session. 'neuron-counts' represents the number of neurons in each session. 'trial_counts" stands for the number of trials in each session.

```{r,echo=FALSE}
suppressMessages(library(knitr))
suppressMessages(library(tidyverse))

n.session=length(session)

meta <- tibble(
  mouse_name = rep('mouse_name',n.session),
  brain_area = rep(0,n.session),
  neuron_counts = rep(0,n.session),
  trial_counts = rep(0,n.session),
  success_rate = rep(0,n.session)
)


for(i in 1:n.session){
  tmp = session[[i]];
  meta[i,1]=tmp$mouse_name;
  meta[i,2]=length(unique(tmp$brain_area));
  meta[i,3]=dim(tmp$spks[[1]])[1];
  meta[i,4]=length(tmp$feedback_type);
  meta[i,5]=mean(tmp$feedback_type+1)/2;
}
trial_num = seq(1, n.session)

meta <- meta %>% add_column(Session_Number = trial_num, .before = 1)
kable(meta, format = "html", table.attr = "class='table table-striped'",digits=2)
```

### ii) Exploring neural activities (average number of failure rate)

In this project, there are two types of feedback: success, represented as 1, and failure, represented as -1. In this subset, I will explore neural activities, specifically focusing on the average rate of failure in each session because it shows mouse's performance on the task. I have generated a table and a plot to present these findings. Each number in the table corresponds to the failure rate for a particular session, ranging between 0 and 1. This represents the percentage of failures. For instance, a failure rate of 0.3947368 indicates that, in the first session, approximately 39.47368% of trials resulted in failure.In the plot, we observe that there is a drastic drop among session 10 and 11 in terms of the average number of failure rate. Other changes seem to be normal.

```{r, echo=FALSE}
suppressMessages(library(tidyverse))
failure_rate_session <- function(session){

  num_failures <- sum(session$feedback_type == -1)

  total_trials <- length(session$feedback_type)

  failure_rate <- num_failures / total_trials

  return(failure_rate)
}

failure_rates <- sapply(session, failure_rate_session)

failure_rates_df <- data.frame(
  session = 1:length(failure_rates),
  failure_rate = failure_rates
)

ggplot(failure_rates_df, aes(x=session, y=failure_rate)) +
  geom_line() +
  geom_point() +
  labs(x="Session", y="Failure Rate") +
  scale_x_continuous(breaks = 1:length(failure_rates)) +  
  theme_minimal()

```

### iii) Changes Across Trials

In this section, I chose to explored changes across trials by examining the cumulative feedback across trials.Cumulative feedback tells whether the mouse is learning a task or not over time. If a mouse is learning, we expect the cumulative feedback to increase as more trials are performed. Examining cumulative feedback also allows use to identify trends and patterns emerged over trials. I observed that there is a drastic drop between around trials 150 to 165. The general trend of cumulative feedback seems to increase gradually over over trials. This might suggest that mouse is having more successful trials than failure trials over time.

```{r,echo=FALSE}

feedback_cumsum_list <- lapply(session, function(x) {
  cumsum(x$feedback_type)
})

plot_list <- list()

for(i in seq_along(feedback_cumsum_list)) {
  df <- data.frame(Trial = 1:length(feedback_cumsum_list[[i]]), Cumulative_Feedback = feedback_cumsum_list[[i]])
  
  p <- ggplot(df, aes(x = Trial, y = Cumulative_Feedback)) +
    geom_line() +
    labs(title = paste("Cumulative Feedback Across Trials for Session", i),
         x = "Trial",
         y = "Cumulative Feedback")
  
  plot_list[[i]] <- p
}

print(plot_list[[2]])
```

### iv) Homogeneity and heterogeneity across sessions and mice (Trial outcomes)

In this section, I will examine the similarities and differences across sessions among mice. To do this, I will assess both the outcomes of trials (success or failure) and the conditions of stimuli (variability in left and right contrasts). Visualizing trial outcomes for the four mice across multiple sessions will reveal whether mice are generally more successful or unsuccessful in completing tasks, and which mouse has a higher success rate than the others. Further, visualization of stimuli conditions across sessions, separated by mouse, will provide boxplots of left and right contrasts for each session. This enables us to identify if a particular mouse is more responsive to specific stimuli in either the left contrast or the right contrast. 

The first plot demonstrates the success and failure rates in task completion for each mouse. Here, 1 represents a success, while -1 represents a failure. The values lie between 0 and 1. We are able to see that all four mice having a success rate of above 0.5 which suggests that they have more success than failure. In particular, the mouse 'lederberg' has the highest success rate among four. We can't say this mouse is in particular smarter than others without know what tasks are assigned to this mouse. I also observed that each mouse is responsible for a certain sessions.

```{r,echo=FALSE}
# Logic:
  # 1: facet_wrap() allows to create seperate plots at once

outcome_df <- data.frame()

for(i in 1:length(session)) {
  temp_df <- data.frame(
    mouse = session[[i]]$mouse_name,
    session = i,
    feedback_type = session[[i]]$feedback_type
  )
  outcome_df <- rbind(outcome_df, temp_df)
}

ggplot(outcome_df, aes(x = factor(session), fill = factor(feedback_type))) +
  geom_bar(position = "fill") +
  facet_wrap(~mouse) +
  labs(x = "Session", y = "Proportion", fill = "Feedback Type") +
  theme_minimal()
```

The Contrasts plot shows a couple of the following aspects: contrast level, mouse type, and left or right contrast. Contrast level represents how much a difference in the levels of visual stimuli being presented to the left and right visual fields of the mice. In the plot, we see that the mouse 'Hench' shows a greater difference in the level of visual sitmuli being presented to the left contrast, and 'cori' shows a smaller difference in the level of visual stimuli being presented to the right contrast. 

```{r,echo=FALSE}
#Logic:
  # 1: facet_wrap() allows to create separate plots at once
  # 2: scales = "free" allows y-axis to take different scales for left         right contrast.
  # 3: nrow = 2 allows to make the plots into a format of rows.
suppressMessages(library(tidyverse))

contrast_df <- data.frame()

# Loop over each session
for(i in 1:length(session)) {
  temp_df <- data.frame(
    mouse = session[[i]]$mouse_name,
    session = i,
    contrast_left = session[[i]]$contrast_left,
    contrast_right = session[[i]]$contrast_right
  )
  contrast_df <- rbind(contrast_df, temp_df)
}

# Reshape data to long format
contrast_df_long <- reshape2::melt(contrast_df, 
                                   id.vars = c("mouse", "session"), 
                                   variable.name = "contrast_type", 
                                   value.name = "contrast_value")

# Visualize contrast conditions
ggplot(contrast_df_long, aes(x = factor(session), y = contrast_value, color = mouse)) +
  geom_boxplot() +
  facet_wrap(~ contrast_type, scales = "free", nrow = 2) +
  labs(x = "Session", y = "Contrast", color = "Mouse") +
  theme_minimal()

```

# Data integration 

In this section, I selected five numeric variables for data integration: feedback_type, contrast_left, contrast_right, total_neurons, spk_mean, and spk_standard_deviation. After generating a sub-data frame, I employed the elbow method to determine the optimal k-value suitable for the K-means clustering method. I chose K-means clustering over other clustering methods because it is well-suited for handling large, multivariate datasets. Following clustering, I performed Principal Component Analysis (PCA) to reduce the dimensionality of the sub-data frame. While PCA is not a requirement, its use can be beneficial in interpreting cluster results when dealing with high-dimensional data, as it simplifies visualization.

```{r,echo=FALSE}
suppressMessages(library(factoextra))
suppressMessages(library(cluster))
suppressMessages(library(tidyverse))
suppressMessages(library(caTools))

session.summary <- list()

for (i in 1:18){
  trial_summary = data.frame(
    session_number = numeric(),
    feedback_type = numeric(),
    contrast_left = numeric(),
    contrast_right = numeric(),
    total_neuron_count = numeric(),
    spk_mean = numeric(),
    spk_standard_deviation = numeric()
  )
  for (j in 1:length(session[[i]]$feedback_type)){
    spks.mean = mean(c(session[[i]]$spks[[j]]))
    spks.sd = sd(c(session[[i]]$spks[[j]]))
    trial_summary = rbind(trial_summary, data.frame(
      session_number = i,
      feedback_type = session[[i]]$feedback_type[[j]],
      contrast_left = session[[i]]$contrast_left[[j]],
      contrast_right = session[[i]]$contrast_right[[j]],
      total_neuron_count = dim(session[[i]]$spks[[1]])[1],
      spks_mean = spks.mean,
      spks_standard_deviation = spks.sd
    ))
  }
  session.summary[[i]] = trial_summary
}
sessions_combined = bind_rows(session.summary)

# Preparing data for k-means clustering
clustering_data <- sessions_combined[, c("contrast_left","contrast_right","spks_mean","spks_standard_deviation","total_neuron_count")]

clustering_data <- scale(clustering_data)

# Perform PCA
pca.result <- prcomp(clustering_data)

# Determine number of clusters using Elbow method
set.seed(123) # for reproducibility
wss <- (nrow(clustering_data)-1)*sum(apply(clustering_data,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(clustering_data, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")

k <- 6 

kmeans_result <- kmeans(clustering_data, centers=k)

sessions_combined$cluster <- kmeans_result$cluster

fviz_cluster(list(data = pca.result$x[, 1:2], cluster = sessions_combined$cluster))
```


# Model training and prediction

In the model training and prediction section, the seed is first set to ensure that the data can be reproduced. Then, the sample.split() method is used to divide the sub-data frame into a training set and a test set. Moving on to the next step, I chose to use glm() to fit a logistic regression model to the training data. Following this, predict() is used to anticipate the feedback type for the test data based on the model. Finally, a confusion matrix and misclassification error rate are generated to display the performance of the model and the proportion of observations that are incorrectly classified or predicted. The result reveals a misclassification error rate of 0.3409449, suggesting that approximately 34% of the observations are incorrectly classified.

```{r, echo=FALSE}
set.seed(123)
split1 = sample.split(sessions_combined$feedback_type, SplitRatio = 0.75, group = NULL)
training_set = sessions_combined[split1, 2:7]
test_set = sessions_combined[!split1, 2:7]

logit_model <- glm(feedback_type ~ contrast_right + spks_mean + spks_standard_deviation + (spks_mean*spks_standard_deviation), data = training_set, family = "gaussian")

estimates_table <- summary(logit_model)$ coef

logit_predictions <- predict(logit_model, newdata = test_set, type = "response")
predicted_class <- ifelse(logit_predictions > 0.6, "-1", "1")
confusion_matrix <- table(Actual = test_set$feedback_type, predicted = predicted_class)
misclassification_rate <- 1 - sum(diag(confusion_matrix))/sum(confusion_matrix)

print("Model parameter estimates and standard errors")
print(estimates_table)
cat("Misclassification Error Rate:", misclassification_rate, "\n")
confusion_matrix
```
# Prediction performance on the test sets

Two test sets are read into the prediction model, and the result shows that about 28.5% of the observations are incorrectly classified. The confusion matrix is interpreted as follow. 0 observations are correctly predicted as -1. 55 observations are wrongly predicted as 1 while in reality, the value is -1. 2 observations are wrongly predicted as -1 while in reality, the value is 1. 143 observations are correctly predicted as 1.

```{r, echo=FALSE}
test=list()
for(i in 1:2){
  test[[i]]=readRDS(paste('~/Downloads/test/test',i,'.rds',sep=''))
}

library(factoextra)
library(cluster)
library(tidyverse)
library(caTools)
test.summary <- list()
for (i in 1:2){
  trial_summary = data.frame(
    test_number = numeric(),
    feedback_type = numeric(),
    contrast_left = numeric(),
    contrast_right = numeric(),
    total_neuron_count = numeric(),
    spk_mean = numeric(),
    spk_standard_deviation = numeric()
  )
  for (j in 1:length(test[[i]]$feedback_type)){
    spks.mean = mean(c(test[[i]]$spks[[j]]))
    spks.sd = sd(c(test[[i]]$spks[[j]]))
    trial_summary = rbind(trial_summary, data.frame(
      test_number = i,
      feedback_type = test[[i]]$feedback_type[[j]],
      contrast_left = test[[i]]$contrast_left[[j]],
      contrast_right = test[[i]]$contrast_right[[j]],
      total_neuron_count = dim(test[[i]]$spks[[1]])[1],
      spks_mean = spks.mean,
      spks_standard_deviation = spks.sd
    ))
  }
  test.summary[[i]] = trial_summary
}
tests_combined = bind_rows(test.summary)

set.seed(123)
split1 = sample.split(tests_combined$feedback_type, SplitRatio = 0.75, group = NULL)
training_set = sessions_combined[split1, 2:7]
test_set = tests_combined

logit_model <- glm(feedback_type ~ contrast_right + spks_mean + spks_standard_deviation + (spks_mean*spks_standard_deviation), data = training_set, family = "gaussian")
estimates_table <- summary(logit_model)$ coef

logit_predictions <- predict(logit_model, newdata = test_set, type = "response")
predicted_class <- ifelse(logit_predictions > 0.6, "-1", "1")
confusion_matrix <- table(Actual = test_set$feedback_type, predicted = predicted_class)
misclassification_rate <- 1 - sum(diag(confusion_matrix))/sum(confusion_matrix)

print("Model parameter estimates and standard errors")
print(estimates_table)
cat("Misclassification Error Rate:", misclassification_rate, "\n")
confusion_matrix
```
# Discussion

Based on the prediction results on the two test sets, it is evident that the model is weak in predicting feedback type -1 (True Negatives), and stronger in predicting feedback type 1 (True Positives). This suggests that the model isn't learning the characteristics of True Negatives very well, a common issue often due to a lack of representative data. Additionally, the model exhibits high False Positives, which could potentially be a serious concern. In essence, the model may exhibit a bias towards predicting 1, possibly due to imbalances in the classes in the training data. For further improvements, I might need to include more numeric variables because additional data provides the predictive model with extra learning opportunities. Furthermore, methods like model tuning by adjusting model parameters might help to enhance the performance of the model.

# Acknowledgment
https://openai.com/blog/chatgpt(chatgpt help on majority of the sections especially part 1 and part 2)

Special thanks to the TAs( Wenzhuo and Chen's) and classmates' help on all parts of my project.)


www.google.com

# Appendix: All code for this report
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
